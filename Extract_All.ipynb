{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def getRawPageDataFromFile(file):\n",
    "    with open(file, 'r') as data_file:\n",
    "        data = json.load(data_file)\n",
    "\n",
    "    return data#[:10080]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'SoupArticle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0171dbd04a5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mSoupArticle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"url\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'SoupArticle'"
     ]
    }
   ],
   "source": [
    "from SoupArticle import *\n",
    "\n",
    "def process_page(page):\n",
    "    url=page[\"url\"]\n",
    "\n",
    "    soup_article = SoupArticle(page[\"body\"])\n",
    "\n",
    "    title = soup_article.getTitle()\n",
    "    location = soup_article.getLocation()\n",
    "    date= soup_article.getDate()\n",
    "    nr = soup_article.getNr()\n",
    "    text = soup_article.getText()\n",
    "\n",
    "    details = {\"url\":url, \"title\":title, \"location\":location, \"date\":date, \"nr\":nr, \"text\":text}\n",
    "\n",
    "    return details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = getRawPageDataFromFile(file=\"data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a list of all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details=[]\n",
    "\n",
    "all_words=[]\n",
    "\n",
    "for p in data:\n",
    "    page_details = process_page(page=p)\n",
    "    \n",
    "    details.append(page_details)\n",
    "    \n",
    "    try:\n",
    "        split_text = page_details[\"text\"].replace(\".\", \" \").replace(\",\",\" \").split(\" \")\n",
    "        \n",
    "        for s in split_text:\n",
    "            all_words.append(s.lower())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error on document:\", page_details)\n",
    "        print(format(e))\n",
    "              \n",
    "all_words=list(set(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_words))\n",
    "all_words[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a list for each length of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "four_five = []\n",
    "six_seven = []\n",
    "eight_nine = []\n",
    "ten_eleven = []\n",
    "twelve_thirteen = []\n",
    "more = []\n",
    "\n",
    "\"\"\"max_word_length = 0\n",
    "for w in all_words:\n",
    "    if len(w)>max_word_length:\n",
    "        max_word_length=len(w)\"\"\"\n",
    "\n",
    "word_len_matrix = {}#[None] * max_word_length\n",
    "\n",
    "def build_lists(d):\n",
    "    for w in d:\n",
    "        \"\"\"if len(w)==4 or len(w)==5:\n",
    "            four_five.append[]\"\"\"\n",
    "        \"\"\"if len(word_len_matrix)+1<len(w):\n",
    "            word_len_matrix.append([])\n",
    "            word_len_matrix[]\"\"\"\n",
    "        #word_len_matrix[len(w)].append(w)\n",
    "        try:\n",
    "            values = word_len_matrix[len(w)]\n",
    "            values.append(w)\n",
    "            #print(values)\n",
    "        except:\n",
    "            values = [w]\n",
    "        \n",
    "        new_dict_part = {len(w):values}\n",
    "        word_len_matrix.update(new_dict_part)\n",
    "\n",
    "build_lists(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_len_matrix[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lookup distances in the lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyxdameraulevenshtein import damerau_levenshtein_distance, normalized_damerau_levenshtein_distance\n",
    "import textdistance\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "#TODO: look up the near rows too\n",
    "#bei Vergleichen mit laengeren, kuerzeren Woertern, darf die Distanz hoch sein, \n",
    "#beim Vergleich mit gleichlangen Woertern eher nicht\n",
    "#bei kuerzeren Woertern, sollte die Distanz generell nur sehr niedrig sein\n",
    "\n",
    "#looks up, which near rows can even exist and returns a list of the valid ones\n",
    "#n - how near\n",
    "#n_letters - to which length\n",
    "#longest_word - don't get higher than this\n",
    "def get_near_rows(n, n_letters, possible_indices):\n",
    "    res = []\n",
    "    offset_near_rows = list(range(n_letters-n, n_letters+n+1))\n",
    "    #offset_near_rows = list(filter(lambda x: x>=0 and x<=longest_word, offset_near_rows))\n",
    "    offset_near_rows = np.intersect1d(offset_near_rows, possible_indices)\n",
    "    \n",
    "    return offset_near_rows\n",
    "        \n",
    "\n",
    "#allowed_distance should be at least equal to n_near_rows\n",
    "def lookup(input_string, allowed_distance, n_results, lookup_list, dist_func, n_near_rows=2):\n",
    "    n_letters = len(input_string)\n",
    "    \n",
    "    #contains the row indices, that are relevant for the lookup\n",
    "    i_lookup_rows = get_near_rows(n_near_rows, n_letters, list(lookup_list.keys()))\n",
    "    \n",
    "    #get the relevant rows for the lookup #+ their distance regarding the length of the words\n",
    "    lookup_rows = []\n",
    "    for i in i_lookup_rows:\n",
    "        row_index_dist = abs(n_letters-i)\n",
    "        #lookup_rows.append((row_index_dist, lookup_list[i]))\n",
    "        lookup_rows.append(lookup_list[i])\n",
    "    \n",
    "    #determine the distance between the given word and every word from the relevant rows\n",
    "    distances_with_words = []\n",
    "    for lookup_row in lookup_rows:\n",
    "        #for w in lookup_row\"\"\"[1]\"\"\":\n",
    "        for w in lookup_row:\n",
    "            dist = dist_func(w, input_string)\n",
    "            #if dist <= allowed_distance+lookup_row[0]:\n",
    "            \"\"\"if dist <= allowed_distance:\"\"\"\n",
    "            distances_with_words.append((dist, w))\n",
    "            \n",
    "    #return distances_with_words\n",
    "    \n",
    "    dtype = [('distance', float), ('word', object)]\n",
    "    distances = np.array(distances_with_words, dtype)\n",
    "    sorted_distances_with_words = np.sort(distances, order='distance')\n",
    "    \n",
    "    if len(sorted_distances_with_words)<=n_results:\n",
    "        return list(sorted_distances_with_words)\n",
    "    return list(sorted_distances_with_words[:n_results])\n",
    "\n",
    "#allowed_distance should be at least equal to n_near_rows\n",
    "def lookup_jaro(input_string, min_sim, n_results, lookup_list, n_near_rows=2, prefix_w=0.1):\n",
    "    n_letters = len(input_string)\n",
    "    \n",
    "    #contains the row indices, that are relevant for the lookup\n",
    "    i_lookup_rows = get_near_rows(n_near_rows, n_letters, list(lookup_list.keys()))\n",
    "    \n",
    "    #get the relevant rows for the lookup + their distance regarding the length of the words\n",
    "    lookup_rows = []\n",
    "    for i in i_lookup_rows:\n",
    "        row_index_dist = abs(n_letters-i)\n",
    "        #lookup_rows.append((row_index_dist, lookup_list[i]))\n",
    "        lookup_rows.append(lookup_list[i])\n",
    "    \n",
    "    #determine the distance between the given word and every word from the relevant rows\n",
    "    distances_with_words = []\n",
    "    #index=0\n",
    "    for lookup_row in lookup_rows:\n",
    "        #for w in lookup_row\"\"\"[1]\"\"\":\n",
    "        for w in lookup_row:\n",
    "            dist = textdistance.JaroWinkler()(w, input_string, prefix_weight=prefix_w)\n",
    "            #if dist <= allowed_distance+lookup_row[0]:\n",
    "            \"\"\"if min_sim <= dist:\"\"\"\n",
    "            distances_with_words.append((dist, w))\n",
    "            \n",
    "    #print(distances_with_words)\n",
    "    dtype = [('distance', float), ('word', object)]\n",
    "    distances = np.array(distances_with_words, dtype)\n",
    "    sorted_distances_with_words = np.sort(distances, order='distance')\n",
    "    \n",
    "    if len(sorted_distances_with_words)<=n_results:\n",
    "        return list(np.flipud(sorted_distances_with_words))\n",
    "    return list(np.flipud(sorted_distances_with_words[len(sorted_distances_with_words)-n_results:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_lookup_time(f1, f2):\n",
    "    t1 = datetime.datetime.now()\n",
    "    res=f1()\n",
    "    t2 = datetime.datetime.now()\n",
    "    d = (t2-t1).total_seconds()*1000\n",
    "    print(\"f1 took\",d,\"milliseconds\")\n",
    "    t1 = datetime.datetime.now()\n",
    "    res=f2()\n",
    "    t2 = datetime.datetime.now()\n",
    "    d = (t2-t1).total_seconds()*1000\n",
    "    print(\"f2 took\",d,\"milliseconds\")\n",
    "    \n",
    "def l1():\n",
    "    #lookup((\"übarfalasd\").lower(), 5, 8, word_len_matrix, dist_func=damerau_levenshtein_distance, n_near_rows=2) \n",
    "    lookup((\"übarfalasd\").lower(), 5, 8, word_len_matrix, dist_func=normalized_damerau_levenshtein_distance, n_near_rows=2)     \n",
    "    #lookup_jaro(\"Drohgen\".lower(), min_sim=0.8, n_results=3, lookup_list=word_len_matrix, n_near_rows=20, prefix_w=0.2)\n",
    "\n",
    "def l2():\n",
    "    #lookup((\"übarfalasd\").lower(), 5, 10, word_len_matrix,  dist_func=damerau_levenshtein_distance, n_near_rows=20)\n",
    "    lookup((\"übarfalasd\").lower(), 5, 8, word_len_matrix, dist_func=normalized_damerau_levenshtein_distance, n_near_rows=20)     \n",
    "    #lookup_jaro(\"Drohgen\".lower(), min_sim=0.8, n_results=3, lookup_list=word_len_matrix, n_near_rows=2, prefix_w=0.2)\n",
    "    \n",
    "compare_lookup_time(l1, l2)\n",
    "\n",
    "def compare(input_s, top):\n",
    "    #print(\"\\nLevenshtein:\\n\", lookup(input_s.lower(), 3, n_results=top, lookup_list=word_len_matrix,  dist_func=textdistance.damerau_levenshtein.distance, n_near_rows=2))\n",
    "    #print(\"\\nLevenshtein:\\n\", lookup(input_s.lower(), 3, n_results=top, lookup_list=word_len_matrix,  dist_func=normalized_damerau_levenshtein_distance, n_near_rows=2))  \n",
    "    print(\"\\nLevenshtein:\\n\", lookup(input_s.lower(), 3, n_results=top, lookup_list=word_len_matrix,  dist_func=normalized_damerau_levenshtein_distance, n_near_rows=2))      \n",
    "    print(\"Jaro Winkler:\\n\", lookup_jaro(input_s.lower(), min_sim=0.8, n_results=top, lookup_list=word_len_matrix, n_near_rows=2, prefix_w=0.2))\n",
    "\n",
    "#lookup((\"übarfa\").lower(), 3, 3, word_len_matrix,  dist_func=damerau_levenshtein_distance, n_near_rows=2)\n",
    "\n",
    "compare(\"überfaö\", 3)#überfall\n",
    "compare(\"übefaö\", 3)\n",
    "compare(\"übarfa\", 3)\n",
    "compare(\"Drohgendil\", 3)\n",
    "compare(\"Drogn\", 3)\n",
    "compare(\"trogen\", 3)\n",
    "compare(\"Polzei\", 3)\n",
    "compare(\"Poliezei\", 3)\n",
    "compare(\"Slägerei\", 3)\n",
    "compare(\"Shlaegerei\", 3)\n",
    "compare(\"Neukoeln\", 3)\n",
    "compare(\"Nuekoeln\", 3)\n",
    "compare(\"Fahead\", 3)\n",
    "compare(\"geklaur\", 3)\n",
    "compare(\"gestoln\", 3)\n",
    "compare(\"fahrzzz\", 10)\n",
    "\n",
    "#lookup((\"überfaö\").lower(), 3, 3, word_len_matrix,  dist_func=textdistance.damerau_levenshtein.distance, n_near_rows=2)\n",
    "#lookup_jaro((\"überfaö\").lower(), min_sim=0.8, n_results=3, lookup_list=word_len_matrix, n_near_rows=2, prefix_w=0.2)\n",
    "\n",
    "#lookup((\"übarfa\").lower(), 1, None, word_len_matrix,  dist_func=textdistance.jaro_winkler.distance, n_near_rows=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"geklaut\" in all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(textdistance.JaroWinkler()(\"fahrend\", \"Fahead\".lower()))\n",
    "print(textdistance.JaroWinkler()(\"fahrrad\", \"Fahead\".lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(damerau_levenshtein_distance(\"fahrxzy\", \"fahrrad\"))\n",
    "print(damerau_levenshtein_distance(\"übarfa\", \"überhaft\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"S\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
